# PPoPP 2026

* [**JanusQuant: Accurate and Efficient 2-bit KV Cache Quantization for Long-Context Inference**](https://doi.org/10.1145/3774934.3786428)
* [**Cacheman: A Comprehensive Last-Level Cache Management System for Multi-tenant Clouds**](https://dl.acm.org/doi/10.1145/3774934.3786415)
* [**Scaling GPU-to-CPU Migration for Efficient Distributed Execution on CPU Clusters**](https://dl.acm.org/doi/10.1145/3774934.3786435)
* [**Laser: Unlocking Layer-Level Scheduling for Efficient Multi-SLO LLM Serving**](https://dl.acm.org/doi/10.1145/3774934.3786413)
* [**TAC: Cache-Based System for Accelerating Billion-Scale GNN Training on Multi-GPU Platform**](https://dl.acm.org/doi/10.1145/3774934.3786460)
* [**FlashAttention-T: Towards Fully Tensorized Attention by Exploiting Tensor-Vector Parallelism**](https://dl.acm.org/doi/10.1145/3774934.3786425)
* [**Accelerating Sparse Transformer Inference on GPU**](https://dl.acm.org/doi/10.1145/3774934.3786434)
* [**MetaAttention: A Unified and Performant Attention Framework across Hardware Backends**](https://dl.acm.org/doi/10.1145/3774934.3786444)